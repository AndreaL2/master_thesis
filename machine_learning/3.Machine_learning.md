
# GWAS with Machine Learning

**Contents:**
- Data preparation
- Machine learning algorithms
  - Logistic Regression
  - Random Forest
  - Gradient Boosting Machine
- Deep Learning
  - Random Grid
  - Deep Neural Networks (DNN)

This process will be shown for data obtained after applying a threshold of 0.2 LD in the IBD analysis as an example.

## Data preparation (train, valid and, test)

We want to obtain train, validation and test set (1/3). We'll check for having 1132 samples for training, 565 samples for validation and, 848 samples for the test set. 


```R
setwd("/home/isglobal.lan/mballesta/data/mballesta/ld02/")
load("train_test.RData")

data02 <- as.h2o(data02, destination_frame="data02")
train.complete <- as.h2o(training, destination_frame = "train_comp")
test.complete <- as.h2o(test, destination_frame = "test_comp")
y <- "cascon"
x <- setdiff(names(data02), y)

data02[,y] <- as.factor(data02[,y])#if not it will calculate regression

splits <- h2o.splitFrame(data02,
                        c(0.50, 0.33),
                        destination_frames = c("train02",
                                               "test02",
                                               "valid02"),
                        seed=123)
sapply(splits, nrow)

test <- splits[[1]]#843
valid <- splits[[2]]#418
train <- splits[[3]]#1284
```

## Machine learning algorithms

* Logistic Regression


```R
glm_fit <- h2o.glm(y=y, 
                   x=x, 
                   training_frame = train.complete, 
                   family = "binomial", 
                   seed = 123)
glm_perf <- h2o.performance(model = glm_fit,
                            newdata = test.complete)

h2o.auc(glm_perf)
h2o.confusionMatrix(glm_perf)

#Visualization options (optional):
plot(glmt_fit, metric = "AUC")
plot(glm_perf, col="red", main = "ROC Curve for Logistic Regression")
```

* Random Forest


```R
forest_fit <- h2o.randomForest(training_frame = train.complete, 
                               x=x, 
                               y=y,
                               ntrees = 100, 
                               seed = 123)
forest_perf <- h2o.performance(model = forest_fit, 
                               newdata = test.complete)

h2o.auc(forest_perf)
h2o.confusionMatrix(forest_perf)
```

* Gradient Boosting Machine (GBM)


```R
gbm_fit <- h2o.gbm(training_frame = train.complete,
                   x=x,
                   y=y, 
                   seed = 123)
gbm_perf <- h2o.performance(model = gbm_fit,
                            newdata = test.complete)

h2o.auc(gbm_perf)
h2o.confusionMatrix(gbm_perf)
```

* K-Nearest Neighbours (KNN)


```R
library(class)
library(ROCR)
fit.knn <- knn(training[-1], 
               test[-1],
               cl=training[,1], 
               k=5)
pred.knn.pred <- prediction(as.numeric(fit.knn), 
                            as.numeric(test[,1]))

#AUC
pred.knn.perf <- performance(pred.knn.pred, "auc")
(AUC.knn <- pred.knn.perf@y.values[[1]])
#confussion matrix:
targets <- rownames(test[test$cascon == 1])
table(test[targets,]$cascon, fit.knn)
```

## Deep Learning

####  Random Grid

Random Grid is used to search for the best values of the hyper-parameters:


```R
g <- h2o.grid("deeplearning",
              search_criteria = list(
              strategy = "RandomDiscrete",
              max_models = 6),
             hyper_params = list(
             seed = 12345,
             l1 = c(0, 1e-6, 3e-6, 1e-5),
             l2 = c(0, 1e-6, 3e-6, 1e-5),
             input_dropout_ratio = c(0, 0.1, 0.2, 0.3),
             hidden_dropout_ratios = list(
                 c(0,0),
                 c(0.2, 0.2),
                 c(0.4, 0.4),
                 c(0.6, 0.6))),
             grid_id = "dl_test",
             x = x,
             y = y,
             hidden = c(200, 200),
             training_frame = train,
             validation_frame = valid,
             activation = "RectifierWithDropout",
             epochs = 0.01)

perf_grid <- h2o.getGrid(grid_id="dl_test", sort_by="auc", decreasing=T)
perf_grid
```

The optimal parameters searched in the grid were the default ones. Overfitting should be check. Validation and test results should not differ:


```R
deep <- h2o.deeplearning(x = x,
                         y = y,
                         training_frame = train,
                         validation_frame = valid, 
                         activation = "Rectifier", 
                         hidden = c(200, 200))
#check overfitting
h2o.scoreHistory(deep)#no overfitting was seen

perf.deep <- h2o.performance(model = deep,
                            newdata = test)
h2o.auc(perf.deep)# validation and test differed a lot
```

#### Deep Neural Networks (DNN)

Data is split in training (2/3) and test (1/3) frame:

```R
#re-load the data and transform to H2OFrame if necessary

splits <- h2o.splitFrame(data02,
                        c(0.80),
                        destination_frames = c("train02",
                                               "test02"),
                        seed=123)
sapply(splits, nrow)

train <- splits[[1]]
test <- splits[[2]]

#y <- "cascon"
#x <- setdiff(names(train), y)

train[,y] <- as.factor(train[,y])
test[,y] <- as.factor(test[,y])
```

Explore for different hidden layers and nodes for the best AUC for instance:


```R
hidden_opt <- NULL
for(i in 1:40) {hidden_opt[[i]] <- rep(20, i*5)}

dl_1 <- h2o.deeplearning(x = x,
                         y = y,
                         training_frame = train,
                         hidden = hidden_opt[[1]],
                         seed = 1,
                         epochs = 50)
perf1 <- h2o.performance(model = dl_1, 
                         newdata = test.complete)
h2o.auc(perf1)
```
